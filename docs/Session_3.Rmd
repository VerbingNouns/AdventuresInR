---
title: "Session 3: Text and corpus data"
output:
  html_notebook:
    toc: yes
    toc_depth: 2
    df_print: paged
    number_sections: true
    toc_float:
      collapsed: false
---

The purpose of this notebook is to demonstrate some of what is possible for the mining and visualisation of a text. We'll go through some of the basic tools and do some data visualisation to demonstrate what is possible. If this kind of analysis is not directly relevant to your research, hopefully you can use this time to practice different data visualisation techniques and become more familiar with seeking out answers to questions about programming R code.

# Setting up

In this session, we'll be using a few packages, but they'll be introduced as we go. Firstly, we'll need to load in `tidyverse` and [`tm`](https://www.tidytextmining.com/) (text mining). We will not explicitly be using [`SnowballC`](https://cran.r-project.org/web/packages/SnowballC/index.html) at this time, but it may be useful to know about as it helps with lemmatization for word frequency analysis in several languages. It is one of `tm`'s dependencies, and may be doing a bit of background work.  

```{r}
library(tidyverse)

# R must be at least 3.3.1 for `tm` and `slam` to work.
# library("SnowballC")
library(tm)

```

Before we get started working with the actual data, we're going to create a custom function for plotting multiple `ggplot` graphs in a single image. This function was copied from [Cookbook for R](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/).

```{r}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

# Reading in corpus

For this demonstration, I've selected a corpus of Shakespeare's plays and adapted some code from [a Kaggle notebook](https://www.kaggle.com/sindhuee/love-in-shakespeare?scriptVersionId=1121270). Once you understand what the code here is doing, it can easily be adapted to a corpus of any structure or content.

We should take a look at how the corpus is structured so we know what we're dealing with.

```{r}
shak <- read.csv("../data/Shakespeare_data.csv",header = TRUE, as.is = TRUE)
```

These are the first ten lines of the corpus.  

```{r}
shak
```

Each column is labeled and the content of the column is consistent for each row (all 111396 of them!). Some of the rows may not be useful. Some contain empty cells (labeled `NA`). Some contain a lot of information and we might need to do some processing on them before we can use the information quantitatively (for example the column `PlayerLine`).

# Word frequency

The first thing we'll look at is word frequency, or how often a string (in this case "love") occurs in the data frame. To do this, we must identify every time the word "love" appears and highlight it in a way so that it can be counted based on different properties of its environment (e.g., by play, by player, by scene, etc).

Here are the first 10 rows of a data frame that contains the number of times "love" appears in each play. It's been sorted in descending order, but doesn't contain any other information about where and when the word occurs.

```{r}
# play level word frequency
plays <- unique(shak$Play)
loveFreq<-numeric()

for (i in 1:length(plays)){
    text <- Corpus(VectorSource(paste(shak[shak$Play==plays[i],]$PlayerLine,collapse=" ")))
    text <- tm_map(text, removePunctuation)
    text <- tm_map(text, PlainTextDocument)
    text <- tm_map(text, removeWords, stopwords('english'))
    
    # stemming to merge all "loved", "loving" into one   
    text <- tm_map(text, stemDocument)
    tdm  <- TermDocumentMatrix(text)
    
    loveFreq[i]<-as.numeric(slam::row_sums(tdm)["love"]) # `slam` must be installed for this to work
  }

lPlay <- data.frame(plays,loveFreq)
lPlay <- na.omit(lPlay)

# order the plays based on the occurence of love
lPlay<-lPlay[order(-lPlay$loveFreq),]

lPlay
```

We can also look at which players say "love" the most over the course of their appearences. These are only the top 10 players who use the word "love" most. 

```{r}
# player level word frequency
players <- unique(shak$Player)
loveFreq <- numeric()

for (i in 1:length(players)){
    text <- Corpus(VectorSource(paste(shak[shak$Player==players[i],]$PlayerLine,collapse=" ")))
    text <- tm_map(text, removePunctuation)
    text <- tm_map(text, PlainTextDocument)
    text <- tm_map(text, removeWords, stopwords('english'))
    text <- tm_map(text, stemDocument)
    
    tdm  <- TermDocumentMatrix(text)
    
    loveFreq[i] <- as.numeric(slam::row_sums(tdm)["love"])
  }

lPlayer <- data.frame(players,loveFreq)
lPlayer <- na.omit(lPlayer)
#order
lPlayer <- lPlayer[order(-lPlayer$loveFreq),]

lPlayer
```

## Exploring someone else's code

Let's go through the for-loop in more detail to understand what it's doing.

**How many players are in this vector?** (Answer: `r length(players)`)  
That's the number of times this for-loop will cycle, each time incrementing the loop number by 1.
```
for (i in 1:length(players)){
```

The first line of the loop uses `Corpus()` and `VectorSource()`. **What do these functions do?**  
We can check the Help files by typing `?VectorSource()`. It looks like both deal with the structure and metadata of a corpus. Importantly, we can see `[i]` in the code though. This is the same `i` from the previous line, which cycles through each of the players in the vector.  
**What does that tell us about the first pass through this loop?**
```
text <- Corpus(VectorSource(paste(shak[shak$Player==players[i],]$PlayerLine,collapse=" ")))
```

The next few lines all use the function `tm_map()`, which performs a transformation on the corpus. In this case, the corpus is the subset we've called `text` in the above line. Judging from the syntax, it appears that they respectively remove punctuation from the text, convert it to a plain text document, remove English stop words, and merge all words with the same stems or lemmas (i.e., "love", "loving", "loved", etc).
```
text <- tm_map(text, removePunctuation)
text <- tm_map(text, PlainTextDocument)
text <- tm_map(text, removeWords, stopwords('english'))
text <- tm_map(text, stemDocument)
```

This line turns the resulting vector into a term-document matrix, which is basically a way of identifying where each word occurs numerically, which will allow us to *count* the words in the next line. 
```
tdm  <- TermDocumentMatrix(text)
```
A simple TDM may look like this:

 | enter | exit | hamlet | ophelia | and
--|--|--|--|--|--
text_1 | 1 | 0 | 1 | 0 | 0
text_2 | 1 | 0 | 0 | 1 | 0
text_3 | 0 | 1 | 1 | 1 | 1

**What might these three "texts" say?**

The last line has several things going on. We can figure out what each function does by trying it on its own.
```
loveFreq[i] <- as.numeric(slam::row_sums(tdm)["love"])
```
First, we can see what `tdm` looks like when `i = 1`:
```{r}
tdm
```

Hmmm, that doesn't look interpretable by a human. Maybe `row_sums()` from the package `slam` will help?
```{r}
slam::row_sums(tdm)
```

Ah ha! That looks more intelligible. **Which player is this for?**  
(Answer = `r players[1]`; if you want to see what different players are doing, you can change the value of `i` by hand and re-run the code.)

So what is `["love"]` doing in the code?
```{r}
slam::row_sums(tdm)["love"]
```

Ah, "love" doesn't exist in this row (where `i = 1`). Let's see what it would look like if we were actually interested in "enter":
```{r}
slam::row_sums(tdm)["enter"]
```

There are two elements that are being printed out here: the word ("enter") and the sum (3). That's what `as.numeric()` will help us with:
```{r}
as.numeric(slam::row_sums(tdm)["enter"])
```

This means that the sum of occurrences of "love" is now entered into index `[i]` in the vector `loveFreq`, and the loop begins again until `i = length(players)`.

Finally, we close off the for-loop.
```
}
```

Can you figure out how to use the code above to create a data frame with the following two conditions?:

* A list of **players**
* Ordered by frequency of the word **death**

```{r}
# create the vectors you need
# create the for-loop to calculate frequency
# create the data frame
# order the data frame from most to least frequent
```

<big>(possible solution below)</big>  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  


```{r}
# create the vectors you need
# players <- unique(shak$Player) # <-- we already made this one before, but we can do it again too
deathFreq <- numeric() # <-- create a vector of class "numeric"

# create the for-loop to calculate frequency
for (i in 1:length(players)){
    text <- Corpus(VectorSource(paste(shak[shak$Player==players[i],]$PlayerLine,collapse=" ")))
    text <- tm_map(text, removePunctuation)
    text <- tm_map(text, PlainTextDocument)
    text <- tm_map(text, removeWords, stopwords('english'))
    text <- tm_map(text,stemDocument)
    
    tdm  <- TermDocumentMatrix(text)
    
    deathFreq[i] <- as.numeric(slam::row_sums(tdm)["death"])
  }

# create the data frame
dPlayer <- data.frame(players,deathFreq)
dPlayer <- na.omit(dPlayer)
# order the data frame from most to least frequent
dPlayer <- dPlayer[order(-dPlayer$deathFreq),]
dPlayer
```

# Visualising a corpus

Maybe you are curious how the longer and shorter plays compare in word frequency. Instead of hand-counting each, we can graph and order them. Based on this graph, you don't need to know exactly how long each is, but you can see that *Othello* is much longer than *Loves Labours Lost*, which can inform how you approach the comparison later on.

```{r}
shak %>%
  # `Play` is the factor we are interested in
  group_by(Play) %>% 
  # summarise the content of `Play` by counting how many instances there are of each (`n`)
  summarise(n = n()) %>% 
  # reorder() allows us to sort them greatest to least
  ggplot(., aes(x=reorder(Play, n),y=n)) + 
    # stat="identity" is crucial for ggplot to know each `n` is the length of the bar
    geom_bar(stat="identity") + 
    # this way, the play names will be horizontal and fully displayed
    coord_flip() + 
    ggtitle("Length of Shakespeare's plays") +
    xlab("Play") +
    ylab("Number of lines")
```

## Perception of frequency

Within a single play, maybe we want to know which characters are the chattiest. We can visualise the number of lines of text per character to get a sense of who is dominating the stage.

```{r}
shak %>%
  filter(Play == "Hamlet") %>% # limit our dataset to only the play "Hamlet"
  group_by(Player) %>%
  summarise(n = n()) %>%
  ggplot(., aes(x=reorder(Player, n),y=n)) +
    geom_bar(stat="identity") +
    coord_flip() +
    ggtitle("Speech in Hamlet") +
    xlab("Player") +
    ylab("Number of lines") #+
    #scale_y_log10()
```

We can also look across plays for frequency. By comparing which plays have the word "love" the most often, we might be able to group them (perceptually) into plays *about love* and those that are not. Maybe?

```{r}
lPlay %>%
  ggplot(., aes(x=reorder(plays, loveFreq),y=loveFreq)) +
    geom_bar(aes(),stat="identity") +
    coord_flip() +
    ggtitle("Love in each play") +
    xlab("Play") +
    ylab("frequency of the word 'love'") +
    theme(legend.position = "none")
```

## Comparing across subsets

One thing that graphs can do very easily is give you a way to identify trends when you sort events (e.g., plays) into multiple different categories. For instance, the frequency graph above is interesting, but there are so many plays and as a non-expert, I can't tell you what each is about, what style it is written in, or whether I'd expect it to be about "love" or not. So, we can add another dimension of information.

```{r}
lPlayCat <- lPlay

# create boolean vectors for four play categories
comedies <- c("A Comedy of Errors",
            "As you like it",
            "Alls well that ends well",
            "Loves Labours Lost",
            "Measure for measure",
            "Merchant of Venice",
            "Merry Wives of Windsor",
            "A Midsummer nights dream",
            "Much Ado about nothing",
            "Taming of the Shrew",
            "Twelfth Night",
            "Two Gentlemen of Verona")
romances <- c("Pericles",
             "Cymbeline",
             "A Winters Tale",
             "The Tempest")
histories <- c("King John",
             "Richard II",
             "Richard III",
             "Henry IV",
             "Henry V",
             "Henry VI Part 1",
             "Henry VI Part 2",
             "Henry VI Part 3",
             "Henry VIII",
             "Coriolanus",
             "Julius Caesar",
             "Antony and Cleopatra",
             "King Lear",
             "macbeth")
tragedies <- c("Titus Andronicus",
             "Romeo and Juliet",
             "Hamlet",
             "Troilus and Cressida",
             "Othello",
             "Timon of Athens")
# create boolean vectors for four play categories
comedy <- lPlayCat$plays %in% comedies
romance <- lPlayCat$plays %in% romances
histories <- lPlayCat$plays %in% histories
tragedy <- lPlayCat$plays %in% tragedies

# create new column with these four categories
lPlayCat <- lPlay %>%
  mutate(category = case_when(comedy == TRUE ~ "comedy",
                              romance == TRUE ~ "romance",
                              history == TRUE ~ "history",
                              tragedy == TRUE ~ "tragedy"))
```

In the following graph, each color represents a different category (as determined by Wikipedia's [First Folio](https://en.wikipedia.org/wiki/First_Folio) page, plus information about the "late romances"). Now, we can see if there are trends for different categories to mention "love" more or less than the others.

````{r}
lPlayCat %>%
  ggplot(., aes(x=reorder(plays, loveFreq),y=loveFreq)) +
    geom_bar(aes(fill=category),stat="identity") +
    coord_flip() +
    ggtitle("Love in each play") +
#    theme(legend.position="none") +
    xlab("Play") +
    ylab("frequency of the word 'love'")
````

It seems to me that <span style="background-color:#F8766D">comedies</span> and <span style="background-color:#C77CFF">tragedies</span> discuss "love" the most, whereas <span style="background-color:#7CAE00">histories</span> and the late <span style="background-color:#00BFC4">romances</span> discuss it the least. Is this intuitive? Maybe. But there's a problem. *A Comedy of Errors* has the fewest mentions of "love", but it's also the shortest play, so it has the fewest words overall. What we really want to see is the proportion of "love"-frequency per play, not the raw counts. To do that, we have to add in the total length of each play to the data frame, but we've lost that information in `lPlayCat` and `lPlay`. **How might you add that information in to the `lPlayCat` dataframe?**

```{r}

```

<big>(possible solution below)</big>  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  



```{r}
# create new data frame for lengths of plays
playLength <- shak %>%
  group_by(Play) %>%
  summarise(n = n()) %>%
  transmute(plays=Play,length=n) # rename columns that we want, which happens to be both of them

# add column to lPlayCat
lPlayCatLen <- lPlayCat %>%
  left_join(.,playLength)
```
`r lPlayCatLen`

Now we can take a look to see how raw and proportional graphs compare.

```{r}
lPlayCatLen %>%
  mutate(proportion = loveFreq/length) %>%
  ggplot(., aes(x=reorder(plays, proportion),y=proportion)) +
    geom_bar(aes(fill=category),stat="identity") +
    coord_flip() +
    ggtitle("Love in each play") +
#    theme(legend.position="none") +
    xlab("Play") +
    ylab("proportional frequency of the word 'love'")

```

Not a whole lot has changed, but I think the distribution of <span style="background-color:#F8766D">comedies</span> and <span style="background-color:#C77CFF">tragedies</span> is even more pronounced. And, we have more information about *A Comedy of Errors*, which is still very close to the bottom of the graph. Not every comedy is about love, it seems.

Finally, we can generate these same types of graphs for different subgroups, too. We'll have to add in `$category` to `shak` as well, but we already have the vectors for each level of `category`, so it's pretty easy to do.

```{r}
# create boolean vectors for four play categories in `shak`
comedy  <- shak$Play %in% comedies
romance <- shak$Play %in% romances
history <- shak$Play %in% histories
tragedy <- shak$Play %in% tragedies

# create new column with these four categories
shakCat <- shak %>%
  mutate(category = case_when(comedy == TRUE ~ "comedy",
                              romance == TRUE ~ "romance",
                              history == TRUE ~ "history",
                              tragedy == TRUE ~ "tragedy"))
```
`r shakCat`

Here's one example, where we look at the number of lines each player has, focusing only on players who have greater than 700 lines. We can also see if there are any trends in these top speakers by play category. It seems to me that the <span style="background-color:#7CAE00">histories</span> dominate, but Hamlet and Iago dominate the scene (so to speak).

```{r}
shakCat %>%
  group_by(Play,Player,category) %>%
  summarise(n = n()) %>%
  filter(n > 700) %>%
  ggplot(., aes(x=reorder(Player, n),y=n)) +
    geom_bar(aes(fill=category),stat="identity") +
    coord_flip() +
    ggtitle("Number of lines by character") +
#    theme(legend.position="none") +
    xlab("Player") +
    ylab("Number of lines")
```

Is this because <span style="background-color:#7CAE00">histories</span> and <span style="background-color:#C77CFF">tragedies</span> tend to be longer plays, overall? Quite possibly:


```{r}
shakCat %>%
  group_by(Play,category) %>%
  summarise(n = n()) %>%
  ggplot(., aes(x=reorder(Play, n),y=n)) +
    geom_bar(aes(fill=category),stat="identity") +
    coord_flip() +
    ggtitle("Length of Shakespeare's plays") +
    theme(legend.position="none") +
    xlab("Play") +
    ylab("Number of lines")
```



# N-grams

```{r}
#install.packages("tidytext")
library(tidytext)
```

Single words might not be able to tell us much about the texts, which is why examining collocations is such a popular technique. What's the difference between "the king", "kill the king", and "kiss the king"? A lot, but we won't know if we only look for instances of "king". There is where *n-grams* become useful. N-grams are sets of adjacent words, calculated by assigning a number to 'n'. That is, if we want sets of two words, we talk about *bigrams*. If we want sets of three words, we talk about *trigrams*. 

## Pre-processing the corpus

First, we can look at the corpus as a list of words, rather than a list of lines (by act and scene). **What is `unnest_tokens()` doing here?**

```{r}
shak %>%
  unnest_tokens(input = PlayerLine, output = word)
```

Now, we can automate the process of counting how often each word occurs. But, of course, certain words are going to be extremely common, and those words are unlikely to be informative.

```{r}
shak %>%
  unnest_tokens(input = PlayerLine, output = word) %>%
  count(word, sort = TRUE)
```

Once we've filtered out our *stop-words*, the most frequent words look quite different.

```{r}
shak %>%
  unnest_tokens(input = PlayerLine, output = word) %>%
  anti_join(stop_words, by = "word") %>%
  count(word, sort = TRUE)
```

However, Shakespeare uses a lot of words that aren't in our default stop-word list, so we can append our own custom list.

```{r}
word <- c(NA,"thou","thee","thy","thine","dost","shalt","wilt","hast","hath","scene","tis","ii","iii","iv","v","vi","vii")
lexicon <- rep("shakespeare",length(word))
new_stop <- cbind(word,lexicon)
shak_stop <- rbind(new_stop,stop_words)
```

Here is a visualisation of how stop words affect the corpus.

```{r fig.width=5, fig.asp=.5, message=FALSE}
p1 <- shak %>%
  as_tibble(.) %>%
  unnest_tokens(tbl=., input = PlayerLine, output = word) %>%
  count(word, sort = TRUE) %>%
  filter(n>800) %>%
  ggplot(., aes(x=reorder(word,n),y=n)) +
    geom_bar(stat="identity") +
    coord_flip() +
    ggtitle("No stop words")
p2 <- shak %>%
  as_tibble(.) %>%
  unnest_tokens(tbl=., input = PlayerLine, output = word) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(n>800) %>%
  ggplot(., aes(x=reorder(word,n),y=n)) +
    geom_bar(stat="identity") +
    coord_flip() +
    ggtitle("Default stop words")
p3 <- shak %>%
  as_tibble(.) %>%
  unnest_tokens(tbl=., input = PlayerLine, output = word) %>%
  anti_join(shak_stop) %>%
  count(word, sort = TRUE) %>%
  filter(n>800) %>%
  ggplot(., aes(x=reorder(word,n),y=n)) +
    geom_bar(stat="identity") +
    coord_flip() +
    ggtitle("Custom stop words")
multiplot(p1,p2,p3,cols=3)
```

## Comparing across plays

Before we try to compare across plays, let's see what the most common bigrams are overall (after being filtered by the custom stop words list).

```{r}
shak %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE)
```

### 1-grams (just regular token frequency)

If we choose a subset of words, we can look at how they are distributed across different plays. In this case, we can compare <span style="background-color:#F8766D">death</span>, <span style="background-color:#7CAE00">king</span>, <span style="background-color:#00BFC4">love</span>, and <span style="background-color:#C77CFF">sweet</span> across six plays. Unsurprisingly, *Romeo and Juliet* uses the word <span style="background-color:#00BFC4">love</span> more than any other play, although *Midsummer Night's Dream* is close. <span style="background-color:#7CAE00">King</span> is also much more common in plays about kings (surprise, surprise).

```{r}
shak[,c(2,5,6)] %>%
  as_tibble() %>%
  unnest_tokens(tbl=., input = PlayerLine, output = word) %>%
  filter(word=="love" | word =="king" | word=="death" | word=="sweet") %>%
  group_by(Player,Play,word) %>%
  summarise(n=n()) %>%
  filter(  Play == "Hamlet" | 
           Play == "King Lear" | 
           Play == "A Midsummer nights dream" | 
           Play == "Othello" | 
           Play == "Henry V" | 
           Play == "Romeo and Juliet") %>%
  arrange(desc(n)) %>%
  ggplot(., aes(x=word,y=n)) +
    geom_bar(aes(fill=word),stat="identity") +
    facet_wrap(~Play)
```

But there are more interesting words you could compare, certainly. These are just one example.

### Bigrams

It's probably much more interesting to look at collocation than simple word frequency. After all, it gives more context. However, it also reduces the number of tokens substantially.

Here, we can visualise three pairs of gendered noun phrases:

Masculine | Feminine
------- | -------
<span style="color:#F8766D">**my lord**</span>    | <span style="color:#00BFC4">**my lady**</span>
<span style="color:#F8766D">**my father**</span>  | <span style="color:#00BFC4">**my mother**</span>
<span style="color:#F8766D">**my husband**</span> | <span style="color:#00BFC4">**my wife**</span>

What kinds of information can we see in the graph?

```{r fig.width=5, fig.asp=.5}
shak %>%
  as_tibble() %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  filter(bigram=="my lord" | bigram =="my lady" | bigram=="my mother" | bigram=="my father" | bigram=="my wife" | bigram=="my husband") %>%
  mutate(gender = bigram) %>%
  mutate(gender = recode_factor(gender,
                `my lord`="masc",
                `my father`="masc",
                `my husband`="masc",
                `my lady`="fem",
                `my mother`="fem",
                `my wife`="fem")) %>%
  group_by(Player,Play,bigram,gender) %>%
  summarise(n=n()) %>%
  mutate(bigramFac = factor(bigram, levels=c("my lord", "my husband", "my father", "my lady", "my wife", "my mother"))) %>%
  # too boring
  filter(  Play != "Henry VI Part 1" &
           Play != "Henry VI Part 2" &
           Play != "Henry VI Part 3" &
           Play != "Pericles" & 
           Play != "Timon of Athens" & 
           Play != "The Tempest") %>%
  # too skewed
  filter(  Play != "Hamlet" &
           Play != "Troilus and Cressida" &
           Play != "Richard III" &
           Play != "Titus Andronicus" & 
           Play != "Henry VIII" & 
           Play != "Much Ado about nothing") %>%
  arrange(desc(n)) %>%
  ggplot(., aes(x=bigramFac,y=n)) +
    geom_bar(aes(fill=gender),stat="identity") +
    scale_y_log10() +
    coord_flip() +
    facet_wrap(~Play,nrow=3)
```


## Networks



```{r}
#install.packages("igraph")
#install.packages("ggraph")
library(igraph)
library(ggraph)
library(grid)

a <- grid::arrow(type = "closed", angle=22.5, length = unit(.1, "inches"))
```

I think the most exciting way to use n-grams is probably network graphs. These graphs show what words co-occur, and in what order. Moreover, they can encode a number of dimensions visually, which would be very difficult to calculate by hand or plot in a more standard quantitative method.

### Bigrams

From the list of bigrams we can generate from our corpus, we can plot a network graph in which the shade of the connection between nodes indicates the frequency of the bigram (darker means more frequent). Moreover, these connections ("edges") are directional, so we can see which order the words are occuring in.

```{r}
shak %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 22) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkblue", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void()
```

We can also use these plots to compare across plays (although token frequency begins to drop precipitously). Here, we can see that <span style="background-color:salmon">Twelfth Night</span> has notable connections between items of clothing (yellow stockings, cross gartered), whereas <span style="background-color:lightblue">Hamlet</span> and <span style="background-color:green2">Romeo and Juliet</span> have more tokens referring to people and their stage directions. <span style="background-color:lightblue">Hamlet</span> additionally has a notable number of "father's death" bigrams, while <span style="background-color:green2">Romeo and Juliet</span> mentions "county Paris" frequently.

```{r fig.width=5, fig.asp=.3}
p1 <- shak %>%
  filter(Play=="Hamlet") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 6) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkblue", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void()

p2 <- shak %>%
  filter(Play == "Twelfth Night") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 6) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkred", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "salmon", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void()

p3 <- shak %>%
  filter(Play == "Romeo and Juliet") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 6) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkgreen", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "green2", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void()

multiplot(p1,p2,p3,cols=3)
```

If we exclude stage directions and compare across six plays, we start to see distinct themes appear. 

```{r fig.width=5, fig.asp=.5}
p1 <- shak %>%
  filter(ActSceneLine != "") %>%
  filter(Play=="Hamlet") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkblue", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void() +
    ggtitle("Hamlet")

p2 <- shak %>%
  filter(ActSceneLine != "") %>%
  filter(Play == "Twelfth Night") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkred", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "salmon", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void() +
    ggtitle("Twelfth Night")

p3 <- shak %>%
  filter(ActSceneLine != "") %>%
  filter(Play == "Romeo and Juliet") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkgreen", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "green2", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void() +
    ggtitle("Romeo and Juliet")

p4 <- shak %>%
  filter(ActSceneLine != "") %>%
  filter(Play == "Othello") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkorange", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "orange", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void() +
    ggtitle("Othello")

p5 <- shak %>%
  filter(ActSceneLine != "") %>%
  filter(Play == "Henry IV") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="cadetblue4", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "cyan", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void() +
    ggtitle("Henry IV")

p6 <- shak %>%
  filter(ActSceneLine != "") %>%
  filter(Play == "The Tempest") %>%
  unnest_tokens(input = PlayerLine, output = bigram, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  count(word1, word2, sort = TRUE) %>%
  filter(n > 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="violet", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "magenta", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void() +
    ggtitle("The Tempest")

multiplot(p1,p2,p3,p4,p5,p6,cols=3)
```

All of these words are found directly adjacent. What about when words are slighting further apart? That is, language is not linear. How can we visualise relationships between topics that are somewhat less immediate?

### Trigrams

One of the simplest ways to visualise more distant connections is the include more words in each network. Here is a graph of trigrams for the entire corpus, excluding stage directions. (Actually, I think this graph may be of only the first two elements of each trigram, but I will have to sort that out later.)

```{r fig.width=5, fig.asp=.5}
shak %>%
  filter(ActSceneLine != "") %>%
  unnest_tokens(input = PlayerLine, output = trigram, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  filter(!word3 %in% shak_stop$word) %>% # filters stop words from third column
  count(word1, word2, word3, sort = TRUE) %>%
  filter(n > 2) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkblue", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel=TRUE) + 
    theme_void()
```


What happens if we treat the first pair and second pair of trigrams as separate bigrams and graph them as before?

We should be able to visualise longer distance relations of these collocations. This is only a taste of what kinds of network graphs can be generated, since the type of graph and content will vary tremendously based on your unique research questions.

```{r fig.width=5, fig.asp=.75}
w1w2 <- shak %>%
  filter(ActSceneLine != "") %>%
  unnest_tokens(input = PlayerLine, output = trigram, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  filter(!word3 %in% shak_stop$word) %>% # filters stop words from third column
  count(word1, word2, word3, sort = TRUE) %>%
  mutate(set = 1) %>%
  transmute(word1=word1,word2=word2,n=n,set=set)
w2w3 <- shak %>%
  filter(ActSceneLine != "") %>%
  unnest_tokens(input = PlayerLine, output = trigram, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% # separates bigram into two columns, one for each word
  filter(!word1 %in% shak_stop$word) %>% # filters stop words from first column
  filter(!word2 %in% shak_stop$word) %>% # filters stop words from second column
  filter(!word3 %in% shak_stop$word) %>% # filters stop words from third column
  count(word1, word2, word3, sort = TRUE) %>%
  mutate(set = 2) %>%
  transmute(word1=word2,word2=word3,n=n,set=set)
wXwY <- bind_rows(w1w2,w2w3)

wXwY %>%
  filter(n>=3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
    geom_node_point(color = "lightblue", size = 5) +
    geom_edge_link(aes(edge_alpha = n), edge_colour="darkblue", show.legend = TRUE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
    geom_node_text(aes(label = name), alpha=.75, repel=TRUE) + # , vjust = 1, hjust = 1) +
    theme_void()
```

# Heatmaps

Another visualisation tool that can be very helpful for showing distributions of events across a structure is the heatmap. A heatmap is a complex histogram, which allows multiple subsets of the data to be compared side-to-side.


In this example, we can look at the variability of length of subsections (acts and scenes) as determined by number of words. First, let's take a look at the numbers. Since there are so many acts and scenes across all the plays, it is unweildy to look at such a table.

```{r}
shak %>%
  filter(ActSceneLine != "") %>%
  mutate(ActSceneLine2 = ActSceneLine) %>%
  separate(ActSceneLine2, c("act", "scene", "line")) %>%
  count(Play,act,scene, sort=TRUE) %>%
  transmute(play=Play, act=as.numeric(act), scene=as.numeric(scene), n=n) %>%
  group_by(play)
```

We could look at a series of histograms to see how plays vary in their distributionâ€¦

```{r}
shak %>%
  filter(ActSceneLine != "") %>%
  mutate(ActSceneLine2 = ActSceneLine) %>%
  separate(ActSceneLine2, c("act", "scene", "line")) %>%
  count(Play,act, sort=TRUE) %>%
  transmute(play=Play, act=as.integer(act), n=n) %>%
  ggplot(aes(x=act)) + 
    geom_histogram(aes(y = n), stat="identity") + facet_wrap(~play)
```


However, it is much easier and quicker to extract this information from a heatmap (although the precise numbers are lost in this version).

```{r fig.width=3, fig.asp=.75}
shak %>%
  filter(ActSceneLine != "") %>%
  mutate(ActSceneLine2 = ActSceneLine) %>%
  separate(ActSceneLine2, c("act", "scene", "line")) %>%
  count(Play,act, sort=TRUE) %>%
  transmute(play=Play, act=as.integer(act), n=n) %>%
  ggplot(aes(x=act,y=reorder(play, n))) + 
    geom_tile(aes(fill = n), colour = "white") + scale_fill_gradient(low = "white", high = "steelblue")
```

Let's focus in on a subset of plays in order to simplify the visualisation. From a simple visual inspection, it appears that **Act V** tends to be the lightest on words, with the very notable exception of *Love's Labour's Lost*. Otherwise, **Act I** is generally fairly heavy on words, and **Act II** tends to be a bit lighter. Armed with that (superficial) observation, we might be able to check whether our eyes deceive us or whether there's something to it. (But I'll leave that exploration for another time.)

```{r}
shak %>%
  filter(Play == "Hamlet" | Play == "King John" | Play == "The Tempest" | 
           Play == "Cymbeline" | Play == "Measure for measure" | Play == "Timon of Athens" | 
           Play == "Richard III" | Play == "Loves Labours Lost" | Play == "A Winters Tale" | 
           Play == "Othello" | Play == "Romeo and Juliet" | Play == "Henry V") %>% 
  filter(ActSceneLine != "") %>%
  mutate(ActSceneLine2 = ActSceneLine) %>%
  separate(ActSceneLine2, c("act", "scene", "line")) %>%
  count(Play,act, sort=TRUE) %>%
  transmute(play=Play, act=as.integer(act), n=n) %>%
  ggplot(aes(x=act,y=reorder(play, n))) + 
    geom_tile(aes(fill = n), colour = "white") + scale_fill_gradient(low = "white", high = "steelblue")
```

The number of scenes per act tends for vary, as does the number of words per scene. Maybe there is some pattern we can detect by plotting the number of words by scene, by act, and by play. This figure is able to communicate a tremendous amount of information in a very small space. Of course, one must still be walked through it to get the full gist of what is shown. The two wordiest scenes in this subset of the corpus are in *Love's Labour's Lost* **Act V, Scene II** (the very last scene in the play) and *A Winter's Tale* **Act IV, Scene IV**. I don't know what happens in these two scenes, but maybe someone who is familiar with the content could identify what makes these two scenes stand out.

```{r fig.width=5, fig.asp=.35}
shak %>%
  filter(Play == "Hamlet" | Play == "King John" | Play == "The Tempest" | 
           Play == "Cymbeline" | Play == "Measure for measure" | Play == "Timon of Athens" | 
           Play == "Richard III" | Play == "Loves Labours Lost" | Play == "A Winters Tale" | 
           Play == "Othello" | Play == "Romeo and Juliet") %>% 
  filter(ActSceneLine != "") %>%
  mutate(ActSceneLine2 = ActSceneLine) %>%
  separate(ActSceneLine2, c("act", "scene", "line")) %>%
  count(Play,act,scene, sort=TRUE) %>%
  transmute(play=Play, act=as.integer(act), scene=as.integer(scene), n=n) %>%
  ggplot(aes(x=scene,y=play)) + 
    geom_tile(aes(fill = n), colour = "white") + 
    scale_fill_gradient(low = "white", high = "red2") +
    scale_x_continuous(breaks=c(0:8)) +
    theme_dark() + 
    facet_wrap(~act, ncol = 5)
```

A similar style figure could also illustrate where each character has the bulk of their lines. Here, we can see Hamlet is positively verbose in **Scene II** in **Acts II, III, V**, but otherwise only marginally wordier than the other players. Moreover, there seems to be the most even distribution of lines (neither very dark nor entirely pale, as colour-coded) in **Act I**, whereas the other acts are quite skewed toward the few main players.

```{r fig.width=5, fig.asp=.5}
shak %>%
  filter(Play == "Hamlet") %>% 
  filter(ActSceneLine != "") %>%
  mutate(ActSceneLine2 = ActSceneLine) %>%
  separate(ActSceneLine2, c("act", "scene", "line")) %>%
  count(Player,act,scene, sort=TRUE) %>%
  transmute(player=Player, act=as.integer(act), scene=as.integer(scene), n=n) %>%
  ggplot(aes(x=scene,y=reorder(player,n))) + 
    geom_tile(aes(fill = n), colour = "white") + 
    scale_fill_gradient(low = "white", high = "red2") +
    scale_x_continuous(breaks=c(0:8)) +
    theme_dark() + 
    facet_wrap(~act, ncol = 5)
```

The same type of plot can be used to examine *Twelfth Night*, which is shorter, more evenly distributed, and much less soliloquy-driven. 

```{r fig.width=5, fig.asp=.5}
shak %>%
  filter(Play == "Twelfth Night") %>% 
  filter(ActSceneLine != "") %>%
  mutate(ActSceneLine2 = ActSceneLine) %>%
  separate(ActSceneLine2, c("act", "scene", "line")) %>%
  count(Player,act,scene, sort=TRUE) %>%
  transmute(player=Player, act=as.integer(act), scene=as.integer(scene), n=n) %>%
  ggplot(aes(x=scene,y=reorder(player,n))) + 
    geom_tile(aes(fill = n), colour = "white") + 
    scale_fill_gradient(low = "white", high = "red2") +
    scale_x_continuous(breaks=c(0:8)) +
    theme_dark() + 
    facet_wrap(~act, ncol = 5)
```

# Wrap up

What this all seems to tell us is that we can visualise the structure of the play, separate from their content. Is this useful to you? It will depend on what kinds of research questions you are interested in. However, if any of your questions involve counting things (words, lines, appearances, collocations, ngrams, etc), then it is possible and even likely that visualisations can help communicate that information in a succinct and easily read way.

Of course, counting things is only going to be helpful for some possible questions. But, it's important to know that not everything that can be counted is a simple number. There are relative frequencies, collocations and n-grams, and changes over "time" (i.e., in different acts, scenes, chapters, by publication year, etc). We can use these relations between numbers to create elegant figures that might or might not communicate numbers specifically (i.e., network graphs). We can then use the figures to explore properties of the text that might not be immediately apparent from close reading.

As a quick (visual) summary of what's been covered, here is a table of plot types and their (best) uses:

Plot | Uses
---|----------
Bar plot | Frequency (relative or absolute)
Histogram | Frequency distribution across a scale
Heatmap | Frequency/attestation across multiple dimensions
Network graph | Collocation (with or without directionality)


Other Plots | Uses
---|----------
Flow chart | Temporal order/branching without regard to spacial orientation
Chord diagram | Connect concepts by frequency but can be busy or confusing
Choropleth map | Map or diagram that uses shading/color to indicate value
Word cloud | *CAUTION:* Pretty but difficult to compare relative sizes  
Pie chart | **DO NOT USE**: Difficult to compare relative size of slices
